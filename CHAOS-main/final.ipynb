{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dense import Dense\n",
    "from activations import *\n",
    "import numpy as np\n",
    "from losses import mse, mse_prime\n",
    "\n",
    "import bisect\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import pandas as pd\n",
    "from parameters import *\n",
    "import copy\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\91993\\\\Desktop\\\\chaos\\\\')\n",
    "home_path = os.getcwd()\n",
    "saved_path = home_path\n",
    "saved_path\n",
    "\n",
    "initialization_list = [i for i in range (1,6)]\n",
    "df_train = {f'train{initialization}':[] for initialization in initialization_list}\n",
    "df_test = {f'test{initialization}':[] for initialization in initialization_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(network):\n",
    "    for items in network:\n",
    "        if items.__class__.__name__ == 'Dense':\n",
    "            print(items.weights)\n",
    "            print(items.bias)\n",
    "\n",
    "\n",
    "def mask_weights(network,mask_list,weights_per_layer):\n",
    "    if len(mask_list)!=0:\n",
    "        for mask in mask_list:\n",
    "            index = bisect.bisect_left(weights_per_layer,mask)\n",
    "            shape_ = network[2*index].weights.shape\n",
    "\n",
    "\n",
    "            if index!=0:\n",
    "                mask = mask - weights_per_layer[index-1]\n",
    "\n",
    "\n",
    "            network[2*index].weights[(mask-1)//shape_[1]][(mask-1)%shape_[1]] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(network, input,store_weights=False,list_w=[],list_b=[]):\n",
    "    output = input\n",
    "    for layer in network:\n",
    "        if layer.__class__.__name__ =='Dense' and store_weights==True:\n",
    "            output = layer.forward(output)\n",
    "            list_w.append(np.copy(layer.weights))\n",
    "            list_b.append(np.copy(layer.bias))\n",
    "        else :\n",
    "            output = layer.forward(output)\n",
    "\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_accuracy(network,X_t,Y_t):\n",
    "    correct = 0\n",
    "    for x, y in zip(X_t, Y_t):\n",
    "        output = predict(network, x,store_weights=False)\n",
    "        if np.argmax(output)==np.argmax(y):\n",
    "            correct +=1\n",
    "\n",
    "    return correct\n",
    "\n",
    "def train(network, loss, loss_prime, x_train, y_train,x_test,y_test, epochs = 1000, learning_rate = 0.01, verbose = True,store_weights=False,mask_weight=False,mask_list=[],weights_per_layer=[],list_w=[],list_b=[]):\n",
    "    test_acc = []\n",
    "    train_acc = []\n",
    "    for e in range(epochs):\n",
    "        error = 0\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            # forward\n",
    "            if mask_weight:\n",
    "                mask_weights(network,mask_list=mask_list,weights_per_layer=weights_per_layer)\n",
    "                \n",
    "            if store_weights :\n",
    "                output = predict(network, x,store_weights=store_weights,list_w=list_w,list_b=list_b)\n",
    "            else:\n",
    "                output = predict(network,x,store_weights=False)\n",
    "            # print(output)\n",
    "            # print(y)\n",
    "            # print(\"\")\n",
    "            # error\n",
    "            error += loss(y, output)\n",
    "\n",
    "            # backward\n",
    "            grad = loss_prime(y, output)\n",
    "            for layer in reversed(network):\n",
    "                grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "            \n",
    "\n",
    "            if store_weights:\n",
    "                train_accuracy = get_accuracy(network,x_train,y_train)\n",
    "                accuracy = get_accuracy(network,x_test,y_test)\n",
    "                test_acc.append(accuracy/x_test.shape[0])\n",
    "                train_acc.append(train_accuracy/x_train.shape[0])\n",
    "\n",
    "        if store_weights!=True:\n",
    "            train_accuracy = get_accuracy(network,x_train,y_train)\n",
    "            accuracy = get_accuracy(network,x_test,y_test)\n",
    "            test_acc.append(accuracy/x_test.shape[0])\n",
    "            train_acc.append(train_accuracy/x_train.shape[0])\n",
    "                \n",
    "        error /= len(x_train)\n",
    "        if verbose:\n",
    "            print(f\"{e + 1}/{epochs}, error={error}\")\n",
    "        \n",
    "\n",
    "    return train_acc,test_acc\n",
    "\n",
    "def visualize(train_acc,test_acc):\n",
    "    plt.plot(train_acc[:])\n",
    "    plt.plot(test_acc[:])\n",
    "    plt.legend(['train','test'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['diabetes-1.npy', 'diabetes-2.npy', 'diabetes-3.npy', 'diabetes-4.npy', 'weight_profiles', 'window_le']\n",
      "initialization already exists\n"
     ]
    }
   ],
   "source": [
    "for init in initialization_list:\n",
    "    home_path = saved_path\n",
    "    os.chdir(home_path)\n",
    "    path = home_path+'\\\\NN-RESULTS-FINAL\\\\datasets'\n",
    "    datasets_path = os.path.join(path,\"data\")\n",
    "    init_path = os.path.join(path,\"initializations\")\n",
    "    os.chdir(datasets_path)\n",
    "\n",
    "\n",
    "    datasets = os.listdir()\n",
    "\n",
    "    #-----------------------------control------------------------------#\n",
    "    data_curr = \"Indian Liver Patient\"\n",
    "    initialization = init\n",
    "    new_init = True\n",
    "    #------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------------------------------#\n",
    "    init_path = os.path.join(path,\"initializations\")\n",
    "    path_initializations = f'{init_path}\\\\{data_curr}'\n",
    "    try:\n",
    "        list_init = os.listdir(path_initializations)\n",
    "    except:\n",
    "        os.mkdir(path_initializations)\n",
    "        list_init = os.listdir(path_initializations)\n",
    "\n",
    "    numbers = []\n",
    "\n",
    "    print(list_init)\n",
    "\n",
    "    for items in list_init:\n",
    "        if(re.search(f'{data_curr}',items)):\n",
    "            str_temp = items.split(\"-\")[1]\n",
    "            numbers.append(int(str_temp.split(\".\")[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if numbers.__contains__(initialization) and new_init:\n",
    "        new_init = False\n",
    "        print(\"initialization already exists\")\n",
    "    else:\n",
    "        if len(numbers)==0:\n",
    "            initialization = 1\n",
    "        else:\n",
    "            initialization = numbers[-1]+1\n",
    "\n",
    "\n",
    "    params = parameter_file(data_curr,path,initialization,new_init)\n",
    "\n",
    "    lr = params['learning_rate'] \n",
    "    epochs = params['num_epochs'] \n",
    "    loss = globals()[params['loss']]\n",
    "    loss_prime = globals()[params['loss']+'_prime']\n",
    "    network = params['network']\n",
    "    start_weights = params['init']\n",
    "\n",
    "    X_train,Y_train,X_test,Y_test = params['data']\n",
    "\n",
    "\n",
    "    num_dense = 0\n",
    "    for items in network:\n",
    "        if items.__class__.__name__ == 'Dense':\n",
    "            num_dense +=1\n",
    "        \n",
    "    if new_init:\n",
    "        init_path = os.path.join(path,\"initializations\")\n",
    "        try:\n",
    "            os.mkdir(f'{init_path}\\\\{data_curr}')\n",
    "\n",
    "        except:\n",
    "            print(\"\")\n",
    "\n",
    "        path_new_init = f'{init_path}\\\\{data_curr}\\\\{data_curr}-{initialization}.npy'\n",
    "        np.save(path_new_init,start_weights)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------#\n",
    "    list_w = []\n",
    "    list_b = []\n",
    "\n",
    "    weights_per_layer = []\n",
    "\n",
    "    for items in network:\n",
    "        if items.__class__.__name__ == 'Dense':\n",
    "            weights_per_layer.append(items.weights.shape[0]*items.weights.shape[1])\n",
    "\n",
    "\n",
    "    sum_ = weights_per_layer[0]\n",
    "    for i in range(1,len(weights_per_layer)):\n",
    "        sum_ += weights_per_layer[i]\n",
    "        weights_per_layer[i] = sum_\n",
    "\n",
    "    train_acc,test_acc  = train(network, loss,loss_prime,X_train,Y_train,X_test,Y_test,epochs=epochs,learning_rate=lr,verbose=False,store_weights=True,mask_weight=False)\n",
    "    df_train[f'train{initialization}'] = train_acc\n",
    "    df_test[f'test{initialization}'] = test_acc\n",
    "\n",
    "        \n",
    "    dict_w = {k:[] for k in range(num_dense)}\n",
    "    dict_b = {k:[] for k in range(num_dense)}\n",
    "\n",
    "    for weights in range(len(list_w)):\n",
    "        dict_w[weights%num_dense].append(list_w[weights])\n",
    "\n",
    "    for bias in range(len(list_b)):\n",
    "        dict_b[bias%num_dense].append(list_b[bias])\n",
    "\n",
    "    for key in dict_w.keys():\n",
    "        for items in range(len(dict_w[key])):\n",
    "            dict_w[key][items] = dict_w[key][items].reshape(1,-1)[0]\n",
    "\n",
    "    for key in dict_w.keys():\n",
    "        for items in range(len(dict_w[key])):\n",
    "            dict_w[key][items] = dict_w[key][items].reshape(1,-1)[0]\n",
    "\n",
    "    df_weights = pd.DataFrame(dict_w[0])\n",
    "\n",
    "    for keys in range(1,num_dense):\n",
    "        df = pd.DataFrame(dict_w[keys])\n",
    "        df_weights = pd.concat([df_weights,df],axis=1)\n",
    "\n",
    "    total_weights = df_weights.shape[1]\n",
    "    new_axis = [k for k in range(total_weights)]\n",
    "    df_weights.set_axis(new_axis,axis=1,inplace=True)\n",
    "\n",
    "\n",
    "    mis_class = [1-testacc for testacc in test_acc]\n",
    "    file_str = f'{path_initializations}\\\\weight_profiles\\\\{data_curr}-{initialization}_weights.csv'\n",
    "    try:\n",
    "        if os.path.exists(file_str):\n",
    "            os.remove(file_str)\n",
    "        df_weights.to_csv(file_str,index=False,header=False)\n",
    "    except:\n",
    "        os.mkdir(f'{path_initializations}\\\\weight_profiles')\n",
    "        df_weights.to_csv(file_str,index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for initialization in initialization_list:    \n",
    "    mis_class = [1-testacc for testacc in df_test[f'test{initialization}']]\n",
    "    ##-----------------------------##\n",
    "    file_str = f'{path_initializations}\\\\weight_profiles\\\\{data_curr}-{initialization}_weights.csv'\n",
    "    window_size = 200\n",
    "    overlap = 0.1\n",
    "\n",
    "    index_val = int(window_size*(1-overlap))\n",
    "    num_iterates = len(mis_class)//index_val\n",
    "    ##-----------------------------##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    curr_dir = os.getcwd()\n",
    "    matlab_dir = home_path+'\\\\NN-RESULTS-FINAL\\\\scripts\\\\matlab\\\\tisean package\\\\TISEAN_3.0.0-windows\\\\Tisean_3.0.0\\\\bin'\n",
    "    os.chdir(matlab_dir)\n",
    "\n",
    "    wpath = home_path+f\"\\\\NN-RESULTS-FINAL\\\\datasets\\\\initializations\\\\{data_curr}\\\\window_le\"\n",
    "\n",
    "    matlab_script = fr'''\n",
    "    close all;\n",
    "    clear all;\n",
    "                                    %make folder from here onwards --> and\n",
    "                                    %the following string should be the same\n",
    "                                    %across all systems wherever it is\n",
    "                                    %supposed to be used\n",
    "\n",
    "\n",
    "    wpath = \"{wpath}\";\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    fullFileName = \"{file_str}\";\n",
    "\n",
    "\n",
    "    data = csvread(fullFileName);\n",
    "    samp = size(data,2);\n",
    "\n",
    "    LE = [];\n",
    "    csvfilename = \"\\\\window_LE_\"+num2str({initialization})+\".csv\";\n",
    "    for m = 1:samp\n",
    "        timeseries = data(:,m);\n",
    "        le = [];\n",
    "        for i = 0:{num_iterates-1}\n",
    "            x = timeseries({index_val}*i+1:{index_val}*(i+1)+{window_size*overlap});\n",
    "            y = L_E(x);\n",
    "        \n",
    "            M = Slope(y);\n",
    "            lslope = verify(M,i);\n",
    "    \n",
    "    \n",
    "            le = [le,lslope];\n",
    "                    \n",
    "        end\n",
    "            LE = [LE;le];\n",
    "    end\n",
    "        csvwrite(wpath+csvfilename,LE);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    function y = L_E(x)\n",
    "        \n",
    "        x=x';\n",
    "        y = [];\n",
    "        y=y';\n",
    "        fid=fopen('logi1.dat','wt');\n",
    "        fprintf(fid,'%f\\n',x);\n",
    "        fclose(fid);\n",
    "\n",
    "        system('lyap_r -m2 -r20 -d1 -s20 -V0 -o logi1_lyap.dat logi1.dat');\n",
    "        y=load('logi1_lyap.dat');\n",
    "\n",
    "    end\n",
    "\n",
    "    function M = Slope(y1)\n",
    "        y_1 = interp1(y1(:,1),y1(:,2),[1 2.5],'linear');\n",
    "        y_2 = interp1(y1(:,1),y1(:,2),[0.75 1],'linear');\n",
    "        y_3= interp1(y1(:,1),y1(:,2),[0.5 0.6],'linear');\n",
    "        y_4= interp1(y1(:,1),y1(:,2),[0.25 0.75],'linear');\n",
    "        \n",
    "        slope1 = (y_1(2)-y_1(1))/(2.5-1);\n",
    "        slope2 = (y_2(2)-y_2(1))/(1-0.75);\n",
    "        slope3 = (y_3(2)-y_3(1))/(0.6-0.5);\n",
    "        slope4 = (y_4(2)-y_4(1))/(.75-.25);\n",
    "        \n",
    "        M = [slope1 , slope2,  slope3 , slope4];\n",
    "\n",
    "    end\n",
    "\n",
    "    function lslope = verify(M,i)\n",
    "        M = round(M,6);\n",
    "        if (M(2) == M(3))\n",
    "            lslope = M(2);\n",
    "        elseif  ( M(3) == M(4)|| M(2) == M(4))\n",
    "            lslope = M(4);  \n",
    "        else\n",
    "            lslope = 0;  \n",
    "            disp(\"Slope not matching at \"+i);\n",
    "        end\n",
    "    end\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    f = open(\"window_LE.m\", \"w\")\n",
    "    f.write(matlab_script)\n",
    "    f.close()\n",
    "\n",
    "    r = 1\n",
    "    r = os.system(r\"matlab -nodisplay -nosplash -nodesktop -r run('window_LE.m'); exit;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dict = {f'mask_list{initialization}':[] for initialization in initialization_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for initialization in initialization_list:\n",
    "    curr_file = f'{path_initializations}\\\\window_le\\\\window_LE_{initialization}.csv'\n",
    "    try:\n",
    "        le = np.genfromtxt(curr_file,delimiter=',')\n",
    "    except:\n",
    "        os.mkdir(f'{path_initializations}\\\\window_le')\n",
    "        le = np.genfromtxt(curr_file,delimiter=',')\n",
    "\n",
    "\n",
    "    le = le.T\n",
    "    df = pd.DataFrame(le)\n",
    "    mis_class = [1-testacc for testacc in df_test[f'test{initialization}']]\n",
    "    mis_risk = []\n",
    "\n",
    "    for i in range(num_iterates):\n",
    "        r = index_val*(i+1) + int((window_size*overlap))\n",
    "        mis_risk.append(mis_class[r-1])\n",
    "\n",
    "    maxlag = 20\n",
    "    pval = []\n",
    "    test = 'ssr_chi2test'\n",
    "\n",
    "    for c in df.columns:\n",
    "        df1 = pd.DataFrame([mis_risk,df.iloc[:,c].values])\n",
    "        df1 = df1.T\n",
    "        non_zero = df1[1].count()\n",
    "        df1 = df1.iloc[:non_zero]\n",
    "        test_result = grangercausalitytests(df1[[0,1]], maxlag=20, verbose=False)\n",
    "        p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "        f_values = [round(test_result[i+1][0][test][0],4) for i in range(maxlag)]\n",
    "        min_p_value = np.min(p_values)\n",
    "        pval.append(min_p_value)\n",
    "    pval = np.asarray(pval)\n",
    "    mask = np.where(pval>=0.05)[0]+1\n",
    "    mask_list = mask.tolist()\n",
    "    mask_dict[f'mask_list{initialization}'] = mask_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sparse = {f'{init}':[]for init in initialization_list}\n",
    "df_test_sparse = {f'{init}':[]for init in initialization_list}\n",
    "df_train_epoch = {f'{init}':[]for init in initialization_list}\n",
    "df_test_epoch = {f'{init}':[]for init in initialization_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for initialization in initialization_list:\n",
    "    params = parameter_file(data_curr,path,initialization,new_init)\n",
    "\n",
    "    lr = params['learning_rate'] \n",
    "    epochs = params['num_epochs'] \n",
    "    loss = globals()[params['loss']]\n",
    "    loss_prime = globals()[params['loss']+'_prime']\n",
    "    network = params['network']\n",
    "    start_weights = params['init']\n",
    "\n",
    "    X_train,Y_train,X_test,Y_test = params['data']\n",
    "\n",
    "    network = initialize_Weights(network,params['init'])\n",
    "    train_sparse, test_sparse = train(network, loss,loss_prime,X_train,Y_train,X_test,Y_test,epochs=epochs,learning_rate=lr,verbose=False,store_weights=False,mask_weight=True,mask_list=mask_list,weights_per_layer=weights_per_layer)\n",
    "    network = initialize_Weights(network,params['init'])\n",
    "    train_acc, test_acc = train(network, loss,loss_prime,X_train,Y_train,X_test,Y_test,epochs=epochs,learning_rate=lr,verbose=False,store_weights=False,mask_weight=False,mask_list=mask_list,weights_per_layer=weights_per_layer)\n",
    "    df_test_sparse[f'{initialization}'] = test_sparse\n",
    "    df_test_epoch[f'{initialization}'] = test_acc\n",
    "\n",
    "    df_train_sparse[f'{initialization}'] = train_sparse\n",
    "    df_train_epoch[f'{initialization}'] = train_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
