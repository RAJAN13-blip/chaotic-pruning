{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dense import Dense\n",
    "from activations import *\n",
    "import numpy as np\n",
    "from losses import mse, mse_prime\n",
    "\n",
    "import bisect\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import pandas as pd\n",
    "from parameters import *\n",
    "import copy\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir('C:\\\\Users\\\\91993\\\\Desktop\\\\chaos\\\\')\n",
    "home_path = os.getcwd()\n",
    "saved_path = home_path\n",
    "saved_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialization_list = [i for i in range(1,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(network):\n",
    "    for items in network:\n",
    "        if items.__class__.__name__ == 'Dense':\n",
    "            print(items.weights)\n",
    "            print(items.bias)\n",
    "\n",
    "\n",
    "def mask_weights(network,mask_list,weights_per_layer):\n",
    "    if len(mask_list)!=0:\n",
    "        for mask in mask_list:\n",
    "            index = bisect.bisect_left(weights_per_layer,mask)\n",
    "            shape_ = network[2*index].weights.shape\n",
    "\n",
    "\n",
    "            if index!=0:\n",
    "                mask = mask - weights_per_layer[index-1]\n",
    "\n",
    "\n",
    "            network[2*index].weights[(mask-1)//shape_[1]][(mask-1)%shape_[1]] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(network, input,store_weights=False,list_w=[],list_b=[]):\n",
    "    output = input\n",
    "    for layer in network:\n",
    "        if layer.__class__.__name__ =='Dense' and store_weights==True:\n",
    "            output = layer.forward(output)\n",
    "            list_w.append(np.copy(layer.weights))\n",
    "            list_b.append(np.copy(layer.bias))\n",
    "        else :\n",
    "            output = layer.forward(output)\n",
    "\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_accuracy(network,X_t,Y_t):\n",
    "    correct = 0\n",
    "    for x, y in zip(X_t, Y_t):\n",
    "        output = predict(network, x,store_weights=False)\n",
    "        if np.argmax(output)==np.argmax(y):\n",
    "            correct +=1\n",
    "\n",
    "    return correct\n",
    "\n",
    "def train(network, loss, loss_prime, x_train, y_train,x_test,y_test, epochs = 1000, learning_rate = 0.01, verbose = True,store_weights=False,mask_weight=False,mask_list=[],weights_per_layer=[],list_w=[],list_b=[]):\n",
    "    test_acc = []\n",
    "    train_acc = []\n",
    "    for e in range(epochs):\n",
    "        error = 0\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            # forward\n",
    "            if mask_weight:\n",
    "                mask_weights(network,mask_list=mask_list,weights_per_layer=weights_per_layer)\n",
    "                \n",
    "            if store_weights :\n",
    "                output = predict(network, x,store_weights=store_weights,list_w=list_w,list_b=list_b)\n",
    "            else:\n",
    "                output = predict(network,x,store_weights=False)\n",
    "            # print(output)\n",
    "            # print(y)\n",
    "            # print(\"\")\n",
    "            # error\n",
    "            error += loss(y, output)\n",
    "\n",
    "            # backward\n",
    "            grad = loss_prime(y, output)\n",
    "            for layer in reversed(network):\n",
    "                grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "            \n",
    "\n",
    "            if store_weights:\n",
    "                train_accuracy = get_accuracy(network,x_train,y_train)\n",
    "                accuracy = get_accuracy(network,x_test,y_test)\n",
    "                test_acc.append(accuracy/x_test.shape[0])\n",
    "                train_acc.append(train_accuracy/x_train.shape[0])\n",
    "\n",
    "        if store_weights!=True:\n",
    "            train_accuracy = get_accuracy(network,x_train,y_train)\n",
    "            accuracy = get_accuracy(network,x_test,y_test)\n",
    "            test_acc.append(accuracy/x_test.shape[0])\n",
    "            train_acc.append(train_accuracy/x_train.shape[0])\n",
    "                \n",
    "        error /= len(x_train)\n",
    "        if verbose:\n",
    "            print(f\"{e + 1}/{epochs}, error={error}\")\n",
    "        \n",
    "\n",
    "    return train_acc,test_acc\n",
    "\n",
    "def visualize(train_acc,test_acc):\n",
    "    plt.plot(train_acc[:])\n",
    "    plt.plot(test_acc[:])\n",
    "    plt.legend(['train','test'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for init in initialization_list:\n",
    "    home_path = saved_path\n",
    "    os.chdir(home_path)\n",
    "    path = home_path+'\\\\NN-RESULTS-FINAL\\\\datasets'\n",
    "    datasets_path = os.path.join(path,\"data\")\n",
    "    init_path = os.path.join(path,\"initializations\")\n",
    "    os.chdir(datasets_path)\n",
    "\n",
    "\n",
    "    datasets = os.listdir()\n",
    "\n",
    "    #-----------------------------control------------------------------#\n",
    "    data_curr = \"binary_sonar\"\n",
    "    initialization = init\n",
    "    new_init = True\n",
    "    #------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------------------------------#\n",
    "    init_path = os.path.join(path,\"initializations\")\n",
    "    path_initializations = f'{init_path}\\\\{data_curr}'\n",
    "    try:\n",
    "        list_init = os.listdir(path_initializations)\n",
    "    except:\n",
    "        os.mkdir(path_initializations)\n",
    "        list_init = os.listdir(path_initializations)\n",
    "\n",
    "    numbers = []\n",
    "\n",
    "    print(list_init)\n",
    "\n",
    "    for items in list_init:\n",
    "        if(re.search(f'{data_curr}',items)):\n",
    "            str_temp = items.split(\"-\")[1]\n",
    "            numbers.append(int(str_temp.split(\".\")[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if numbers.__contains__(initialization) and new_init:\n",
    "        new_init = False\n",
    "        print(\"initialization already exists\")\n",
    "    else:\n",
    "        if len(numbers)==0:\n",
    "            initialization = 1\n",
    "        else:\n",
    "            initialization = numbers[-1]+1\n",
    "\n",
    "    #====================Data and Network Initialization======================================#\n",
    "    params = parameter_file(data_curr,path,initialization,new_init)\n",
    "\n",
    "    lr = params['learning_rate'] \n",
    "    epochs = params['num_epochs'] \n",
    "    loss = globals()[params['loss']]\n",
    "    loss_prime = globals()[params['loss']+'_prime']\n",
    "    network = params['network']\n",
    "    network_list.append(network)\n",
    "    start_weights = params['init']\n",
    "\n",
    "    X_train,Y_train,X_test,Y_test = params['data']\n",
    "    #=========================================================================================#\n",
    "    if new_init:\n",
    "        init_path = os.path.join(path,\"initializations\")\n",
    "        try:\n",
    "            os.mkdir(f'{init_path}\\\\{data_curr}')\n",
    "\n",
    "        except:\n",
    "            print(\"\")\n",
    "\n",
    "        path_new_init = f'{init_path}\\\\{data_curr}\\\\{data_curr}-{initialization}.npy'\n",
    "        np.save(path_new_init,start_weights)\n",
    "\n",
    "    weights_per_layer = []\n",
    "\n",
    "    for items in network:\n",
    "        if items.__class__.__name__ == 'Dense':\n",
    "            weights_per_layer.append(items.weights.shape[0]*items.weights.shape[1])\n",
    "\n",
    "\n",
    "    sum_ = weights_per_layer[0]\n",
    "    for i in range(1,len(weights_per_layer)):\n",
    "        sum_ += weights_per_layer[i]\n",
    "        weights_per_layer[i] = sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sparse = {f'{init}':[]for init in initialization_list}\n",
    "df_test_sparse = {f'{init}':[]for init in initialization_list}\n",
    "df_train_epoch = {f'{init}':[]for init in initialization_list}\n",
    "df_test_epoch = {f'{init}':[]for init in initialization_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights(network_list[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network = np.copy(network_list[8])\n",
    "initialization = 1\n",
    "# train_sparse, test_sparse = train(network, loss,loss_prime,X_train,Y_train,X_test,Y_test,epochs=epochs,learning_rate=lr,verbose=False,store_weights=False,mask_weight=True,mask_list=mask_list,weights_per_layer=weights_per_layer)\n",
    "# network = initialize_Weights(network,params['init'])\n",
    "train_acc, test_acc = train(network, loss,loss_prime,X_train,Y_train,X_test,Y_test,epochs=50,learning_rate=0.2,verbose=False,store_weights=False,mask_weight=False,mask_list=[],weights_per_layer=weights_per_layer)\n",
    "# df_test_sparse[f'{initialization}'] = test_sparse\n",
    "df_test_epoch[f'{initialization}'] = test_acc\n",
    "\n",
    "# df_train_sparse[f'{initialization}'] = train_sparse\n",
    "df_train_epoch[f'{initialization}'] = train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(df_train_epoch[f'{initialization}'],df_test_epoch[f'{initialization}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(df_train_epoch[f'{initialization}'],df_test_epoch[f'{initialization}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(df_train_epoch[f'{initialization}'],df_test_epoch[f'{initialization}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "testing for pytorch custom model \n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms \n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class MNIST(nn.Module):\n",
    "    def __init__(self,input_size, num_classes):\n",
    "        super(MNIST,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,15)\n",
    "        self.fc2 = nn.Linear(15,30)\n",
    "        self.fc3 = nn.Linear(30,10)\n",
    "        self.fc4 = nn.Linear(10,num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "\n",
    "    def mask(self,model_tensor): \n",
    "        self.fc1.weight = torch.nn.parameter.Parameter(model_tensor['fc1']*self.fc1.weight)\n",
    "        self.fc2.weight = torch.nn.parameter.Parameter(model_tensor['fc2']*self.fc2.weight)\n",
    "        self.fc3.weight = torch.nn.parameter.Parameter(model_tensor['fc3']*self.fc3.weight)\n",
    "        self.fc4.weight = torch.nn.parameter.Parameter(model_tensor['fc4']*self.fc4.weight)\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, datapath,train,size):\n",
    "        self.data = pd.read_csv(datapath)\n",
    "        arr_data = np.array(self.data,dtype=np.float64)\n",
    "        \n",
    "        if train==True:\n",
    "            arr_data = arr_data[:int(size*len(arr_data))]\n",
    "        else : \n",
    "            arr_data = arr_data[int(size*len(arr_data)):]\n",
    "        self.arr_data = (arr_data)\n",
    "        self.x = torch.from_numpy(self.arr_data[:,1:])\n",
    "        self.x = self.x.float()\n",
    "        self.y = torch.from_numpy(self.arr_data[:,0])\n",
    "        self.y = self.y.float()\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.arr_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index], F.one_hot(self.y[index].long(),10)\n",
    "\n",
    "\n",
    "def model_summary(model):\n",
    "    \"\"\"\n",
    "    get each layer weights, keys and shapes\n",
    "    \"\"\"\n",
    "    assert isinstance(model,nn.Module)==True\n",
    "\n",
    "    model_keys = []\n",
    "    model_shapes = []\n",
    "    for name, params in model.named_parameters():\n",
    "        if name.split(\".\")[1]==\"weight\":\n",
    "            model_keys.append(name.split(\".\")[0])\n",
    "            model_shapes.append(params.shape[0]*params.shape[1])\n",
    "\n",
    "    return model_keys,model_shapes\n",
    "\n",
    "\n",
    "\n",
    "def get_weights(model:nn.Module,model_dict):\n",
    "    \"\"\"\n",
    "    check whether model is instance of nn.Module\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if name.split(\".\")[1]==\"weight\":\n",
    "            index = name.split(\".\")[0]\n",
    "            temp_tensor = param.clone()\n",
    "            temp_weights = temp_tensor.detach().cpu().numpy()\n",
    "            temp_weights = temp_weights.reshape(-1,)\n",
    "            \n",
    "            model_dict[f'{index}'] = np.vstack((model_dict[f'{index}'],temp_weights))\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_weights(model:nn.Module,name):\n",
    "    \"\"\"\n",
    "    check whether model is instance of nn.Module\n",
    "    \"\"\"\n",
    "    W = []\n",
    "    for layer, param in model.named_parameters():\n",
    "        if layer.split(\".\")[1]==\"weight\":\n",
    "            # index = name.split(\".\")[0]\n",
    "            temp_tensor = param.clone()\n",
    "            temp_weights = temp_tensor.detach().cpu().numpy()\n",
    "            temp_weights = temp_weights.reshape(-1)\n",
    "            W.extend(temp_weights.tolist())\n",
    "\n",
    "    with open(name,'a',newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerow(W)\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_accuracy(model:nn.Module, name,dataloader,device,save_weights = False):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data,targets in dataloader:\n",
    "            data = data.to(device)\n",
    "            \n",
    "            targets = targets.to(device)\n",
    "            outputs = model(data)\n",
    "            predictions = torch.argmax(outputs,1)\n",
    "            targets = torch.argmax(targets,1)\n",
    "            n_samples += targets.shape[0]\n",
    "            n_correct += (predictions == targets).sum().tolist()\n",
    "\n",
    "    acc = 100*n_correct/n_samples\n",
    "    if save_weights:\n",
    "        with open(name,'a',newline='') as file:\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            writer.writerow([acc])\n",
    "        file.close()\n",
    "    else:\n",
    "        return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n",
      "11500 41000\n",
      "epoch 1 / 50, step 200/230, loss = 0.0882\n",
      "0\n",
      "train 20.269396918262526\n",
      "test 20.33965558289025\n",
      "epoch 2 / 50, step 200/230, loss = 0.0873\n",
      "1\n",
      "train 24.796761794618863\n",
      "test 25.085310689627807\n",
      "epoch 3 / 50, step 200/230, loss = 0.0799\n",
      "2\n",
      "train 36.576074016122995\n",
      "test 37.012935481311004\n",
      "epoch 4 / 50, step 200/230, loss = 0.0801\n",
      "3\n",
      "train 46.675056974727035\n",
      "test 47.20260296801841\n",
      "epoch 5 / 50, step 200/230, loss = 0.0753\n",
      "4\n",
      "train 48.60709547943808\n",
      "test 49.09927783509245\n",
      "epoch 6 / 50, step 200/230, loss = 0.0686\n",
      "5\n",
      "train 49.341814347426784\n",
      "test 49.77382747401\n",
      "epoch 7 / 50, step 200/230, loss = 0.0613\n",
      "6\n",
      "train 54.18211503792646\n",
      "test 54.55122609316721\n",
      "epoch 8 / 50, step 200/230, loss = 0.0538\n",
      "7\n",
      "train 62.845675022959966\n",
      "test 62.96325688437425\n",
      "epoch 9 / 50, step 200/230, loss = 0.0477\n",
      "8\n",
      "train 73.13173917480187\n",
      "test 72.67677168478693\n",
      "epoch 10 / 50, step 200/230, loss = 0.0422\n",
      "9\n",
      "train 78.56049525494065\n",
      "test 78.0176176493929\n",
      "epoch 11 / 50, step 200/230, loss = 0.0357\n",
      "10\n",
      "train 82.22048368992142\n",
      "test 81.82683913975082\n",
      "epoch 12 / 50, step 200/230, loss = 0.0340\n",
      "11\n",
      "train 83.23752508588727\n",
      "test 82.64423458455678\n",
      "epoch 13 / 50, step 200/230, loss = 0.0296\n",
      "12\n",
      "train 84.80900710908534\n",
      "test 84.37425601142766\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 77\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     76\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mfor\u001b[39;00m i,(data,targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     78\u001b[0m \u001b[39m#       weights[j] = get_weights(model=model)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m         j\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     80\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/chaos_net/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/chaos_net/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/chaos_net/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/chaos_net/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Projects/CHAOS/torch_custom.py:62\u001b[0m, in \u001b[0;36mMNISTDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m,index):\n\u001b[0;32m---> 62\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx[index], F\u001b[39m.\u001b[39;49mone_hot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my[index]\u001b[39m.\u001b[39;49mlong(),\u001b[39m10\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch_custom import *\n",
    "import tqdm \n",
    "\n",
    "\n",
    "from torch_custom import *\n",
    "import tqdm \n",
    "\n",
    "# start from 1\n",
    "initialization = 1\n",
    "torch.manual_seed(initialization)\n",
    "np.random.seed(initialization)\n",
    "\n",
    "\n",
    "# initialization = 5 seed = 0\n",
    "\n",
    "name = f\"weights{initialization}.csv\"\n",
    "train_acc = f\"train_accuracies{initialization}.csv\"\n",
    "test_acc = f\"test_accuracies{initialization}.csv\"\n",
    "\n",
    "datapath = r\"mnist.csv\"\n",
    "\n",
    "\"\"\"\n",
    "MODEL INTITIALIZATION AND SETTING UP THE HYPERPARAMETERS\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "model = MNIST(784,10).to(device) #change the model definition from the torch_custom file to test on other datasets\n",
    "train_size = 0.7\n",
    "datasets = MNISTDataset(datapath,train=True,size=train_size) #change the dataset class from the torch_custom file\n",
    "test_datasets = MNISTDataset(datapath,train=False,size=train_size)\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset=datasets,batch_size=batch_size,shuffle=True)\n",
    "testloader = DataLoader(dataset=test_datasets,batch_size=batch_size)\n",
    "\n",
    "num_epochs = 50\n",
    "learning_rate = 0.5\n",
    "\n",
    "n_total_steps = len(dataloader)\n",
    "print(n_total_steps)\n",
    "\n",
    "torch.save(model.state_dict(), f\"model_{initialization}.pth\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "\n",
    "w_num = 0\n",
    "model_keys, model_shapes = model_summary(model)\n",
    "for shapes in model_shapes:\n",
    "    w_num += shapes             #shapes[0]*shapes[1]\n",
    "#weights = np.zeros(shape=(n_total_steps*num_epochs,w_num))\n",
    "print(n_total_steps*num_epochs,w_num)\n",
    "\n",
    "acc = []\n",
    "for i in range(0,25):\n",
    "    r = 45*(i+1)+5\n",
    "    acc.append(r)\n",
    "\n",
    "\n",
    "j = 0\n",
    "\n",
    "\"\"\"\n",
    "TRAINING AND VALIDATION LOOPS \n",
    "\"\"\"\n",
    "prof = torch.profiler.profile(\n",
    "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/torch_custom'),\n",
    "        record_shapes=True,profile_memory=True,\n",
    "        with_stack=True)\n",
    "prof.start()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    " \n",
    "    model.train()\n",
    "    for i,(data,targets) in enumerate(dataloader):\n",
    "#       weights[j] = get_weights(model=model)\n",
    "        j+=1\n",
    "        data = data.to(device)\n",
    "        temp_targets = targets\n",
    "        targets = targets.float()\n",
    "\n",
    "        targets = targets.to(device)\n",
    "   \n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs,targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if(i+1)%200==0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
    "#        if epoch*n_total_steps+i in acc:\n",
    "#            get_accuracy(model=model,name =train_acc,dataloader=dataloader,device=device,save_weights=True)\n",
    "#            get_accuracy(model=model,name = test_acc,dataloader=testloader,device=device,save_weights=True)\n",
    "    print(epoch)\n",
    "    print(\"train\",get_accuracy(model=model,name =train_acc,device=device,dataloader=dataloader))\n",
    "    print(\"test\",get_accuracy(model=model,name = test_acc,device=device,dataloader=testloader))\n",
    "\n",
    "\n",
    "prof.stop()\n",
    "#np.savetxt(f\"weights{initialization}.csv\",weights,delimiter=',')\n",
    "#weights = pd.DataFrame(weights)\n",
    "#weights.to_csv(f\"weights{initialization}.csv\",header=False,index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chaos",
   "language": "python",
   "name": "chaos_net"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
