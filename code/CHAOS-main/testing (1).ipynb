{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dense import Dense\n",
    "from activations import *\n",
    "import numpy as np\n",
    "from losses import mse, mse_prime\n",
    "\n",
    "import bisect\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import pandas as pd\n",
    "from parameters import *\n",
    "import copy\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir('C:\\\\Users\\\\91993\\\\Desktop\\\\chaos\\\\')\n",
    "home_path = os.getcwd()\n",
    "saved_path = home_path\n",
    "saved_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialization_list = [i for i in range(1,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(network):\n",
    "    for items in network:\n",
    "        if items.__class__.__name__ == 'Dense':\n",
    "            print(items.weights)\n",
    "            print(items.bias)\n",
    "\n",
    "\n",
    "def mask_weights(network,mask_list,weights_per_layer):\n",
    "    if len(mask_list)!=0:\n",
    "        for mask in mask_list:\n",
    "            index = bisect.bisect_left(weights_per_layer,mask)\n",
    "            shape_ = network[2*index].weights.shape\n",
    "\n",
    "\n",
    "            if index!=0:\n",
    "                mask = mask - weights_per_layer[index-1]\n",
    "\n",
    "\n",
    "            network[2*index].weights[(mask-1)//shape_[1]][(mask-1)%shape_[1]] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(network, input,store_weights=False,list_w=[],list_b=[]):\n",
    "    output = input\n",
    "    for layer in network:\n",
    "        if layer.__class__.__name__ =='Dense' and store_weights==True:\n",
    "            output = layer.forward(output)\n",
    "            list_w.append(np.copy(layer.weights))\n",
    "            list_b.append(np.copy(layer.bias))\n",
    "        else :\n",
    "            output = layer.forward(output)\n",
    "\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_accuracy(network,X_t,Y_t):\n",
    "    correct = 0\n",
    "    for x, y in zip(X_t, Y_t):\n",
    "        output = predict(network, x,store_weights=False)\n",
    "        if np.argmax(output)==np.argmax(y):\n",
    "            correct +=1\n",
    "\n",
    "    return correct\n",
    "\n",
    "def train(network, loss, loss_prime, x_train, y_train,x_test,y_test, epochs = 1000, learning_rate = 0.01, verbose = True,store_weights=False,mask_weight=False,mask_list=[],weights_per_layer=[],list_w=[],list_b=[]):\n",
    "    test_acc = []\n",
    "    train_acc = []\n",
    "    for e in range(epochs):\n",
    "        error = 0\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            # forward\n",
    "            if mask_weight:\n",
    "                mask_weights(network,mask_list=mask_list,weights_per_layer=weights_per_layer)\n",
    "                \n",
    "            if store_weights :\n",
    "                output = predict(network, x,store_weights=store_weights,list_w=list_w,list_b=list_b)\n",
    "            else:\n",
    "                output = predict(network,x,store_weights=False)\n",
    "            # print(output)\n",
    "            # print(y)\n",
    "            # print(\"\")\n",
    "            # error\n",
    "            error += loss(y, output)\n",
    "\n",
    "            # backward\n",
    "            grad = loss_prime(y, output)\n",
    "            for layer in reversed(network):\n",
    "                grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "            \n",
    "\n",
    "            if store_weights:\n",
    "                train_accuracy = get_accuracy(network,x_train,y_train)\n",
    "                accuracy = get_accuracy(network,x_test,y_test)\n",
    "                test_acc.append(accuracy/x_test.shape[0])\n",
    "                train_acc.append(train_accuracy/x_train.shape[0])\n",
    "\n",
    "        if store_weights!=True:\n",
    "            train_accuracy = get_accuracy(network,x_train,y_train)\n",
    "            accuracy = get_accuracy(network,x_test,y_test)\n",
    "            test_acc.append(accuracy/x_test.shape[0])\n",
    "            train_acc.append(train_accuracy/x_train.shape[0])\n",
    "                \n",
    "        error /= len(x_train)\n",
    "        if verbose:\n",
    "            print(f\"{e + 1}/{epochs}, error={error}\")\n",
    "        \n",
    "\n",
    "    return train_acc,test_acc\n",
    "\n",
    "def visualize(train_acc,test_acc):\n",
    "    plt.plot(train_acc[:])\n",
    "    plt.plot(test_acc[:])\n",
    "    plt.legend(['train','test'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for init in initialization_list:\n",
    "    home_path = saved_path\n",
    "    os.chdir(home_path)\n",
    "    path = home_path+'\\\\NN-RESULTS-FINAL\\\\datasets'\n",
    "    datasets_path = os.path.join(path,\"data\")\n",
    "    init_path = os.path.join(path,\"initializations\")\n",
    "    os.chdir(datasets_path)\n",
    "\n",
    "\n",
    "    datasets = os.listdir()\n",
    "\n",
    "    #-----------------------------control------------------------------#\n",
    "    data_curr = \"binary_sonar\"\n",
    "    initialization = init\n",
    "    new_init = True\n",
    "    #------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------------------------------#\n",
    "    init_path = os.path.join(path,\"initializations\")\n",
    "    path_initializations = f'{init_path}\\\\{data_curr}'\n",
    "    try:\n",
    "        list_init = os.listdir(path_initializations)\n",
    "    except:\n",
    "        os.mkdir(path_initializations)\n",
    "        list_init = os.listdir(path_initializations)\n",
    "\n",
    "    numbers = []\n",
    "\n",
    "    print(list_init)\n",
    "\n",
    "    for items in list_init:\n",
    "        if(re.search(f'{data_curr}',items)):\n",
    "            str_temp = items.split(\"-\")[1]\n",
    "            numbers.append(int(str_temp.split(\".\")[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if numbers.__contains__(initialization) and new_init:\n",
    "        new_init = False\n",
    "        print(\"initialization already exists\")\n",
    "    else:\n",
    "        if len(numbers)==0:\n",
    "            initialization = 1\n",
    "        else:\n",
    "            initialization = numbers[-1]+1\n",
    "\n",
    "    #====================Data and Network Initialization======================================#\n",
    "    params = parameter_file(data_curr,path,initialization,new_init)\n",
    "\n",
    "    lr = params['learning_rate'] \n",
    "    epochs = params['num_epochs'] \n",
    "    loss = globals()[params['loss']]\n",
    "    loss_prime = globals()[params['loss']+'_prime']\n",
    "    network = params['network']\n",
    "    network_list.append(network)\n",
    "    start_weights = params['init']\n",
    "\n",
    "    X_train,Y_train,X_test,Y_test = params['data']\n",
    "    #=========================================================================================#\n",
    "    if new_init:\n",
    "        init_path = os.path.join(path,\"initializations\")\n",
    "        try:\n",
    "            os.mkdir(f'{init_path}\\\\{data_curr}')\n",
    "\n",
    "        except:\n",
    "            print(\"\")\n",
    "\n",
    "        path_new_init = f'{init_path}\\\\{data_curr}\\\\{data_curr}-{initialization}.npy'\n",
    "        np.save(path_new_init,start_weights)\n",
    "\n",
    "    weights_per_layer = []\n",
    "\n",
    "    for items in network:\n",
    "        if items.__class__.__name__ == 'Dense':\n",
    "            weights_per_layer.append(items.weights.shape[0]*items.weights.shape[1])\n",
    "\n",
    "\n",
    "    sum_ = weights_per_layer[0]\n",
    "    for i in range(1,len(weights_per_layer)):\n",
    "        sum_ += weights_per_layer[i]\n",
    "        weights_per_layer[i] = sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_sparse = {f'{init}':[]for init in initialization_list}\n",
    "df_test_sparse = {f'{init}':[]for init in initialization_list}\n",
    "df_train_epoch = {f'{init}':[]for init in initialization_list}\n",
    "df_test_epoch = {f'{init}':[]for init in initialization_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights(network_list[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network = np.copy(network_list[8])\n",
    "initialization = 1\n",
    "# train_sparse, test_sparse = train(network, loss,loss_prime,X_train,Y_train,X_test,Y_test,epochs=epochs,learning_rate=lr,verbose=False,store_weights=False,mask_weight=True,mask_list=mask_list,weights_per_layer=weights_per_layer)\n",
    "# network = initialize_Weights(network,params['init'])\n",
    "train_acc, test_acc = train(network, loss,loss_prime,X_train,Y_train,X_test,Y_test,epochs=50,learning_rate=0.2,verbose=False,store_weights=False,mask_weight=False,mask_list=[],weights_per_layer=weights_per_layer)\n",
    "# df_test_sparse[f'{initialization}'] = test_sparse\n",
    "df_test_epoch[f'{initialization}'] = test_acc\n",
    "\n",
    "# df_train_sparse[f'{initialization}'] = train_sparse\n",
    "df_train_epoch[f'{initialization}'] = train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(df_train_epoch[f'{initialization}'],df_test_epoch[f'{initialization}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(df_train_epoch[f'{initialization}'],df_test_epoch[f'{initialization}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(df_train_epoch[f'{initialization}'],df_test_epoch[f'{initialization}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "testing for pytorch custom model \n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms \n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class MNIST(nn.Module):\n",
    "    def __init__(self,input_size, num_classes):\n",
    "        super(MNIST,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,15)\n",
    "        self.fc2 = nn.Linear(15,30)\n",
    "        self.fc3 = nn.Linear(30,10)\n",
    "        self.fc4 = nn.Linear(10,num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x \n",
    "\n",
    "    def mask(self,model_tensor): \n",
    "        self.fc1.weight = torch.nn.parameter.Parameter(model_tensor['fc1']*self.fc1.weight)\n",
    "        self.fc2.weight = torch.nn.parameter.Parameter(model_tensor['fc2']*self.fc2.weight)\n",
    "        self.fc3.weight = torch.nn.parameter.Parameter(model_tensor['fc3']*self.fc3.weight)\n",
    "        self.fc4.weight = torch.nn.parameter.Parameter(model_tensor['fc4']*self.fc4.weight)\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, datapath,train,size):\n",
    "        self.data = pd.read_csv(datapath)\n",
    "        arr_data = np.array(self.data,dtype=np.float64)\n",
    "        \n",
    "        if train==True:\n",
    "            arr_data = arr_data[:int(size*len(arr_data))]\n",
    "        else : \n",
    "            arr_data = arr_data[int(size*len(arr_data)):]\n",
    "        self.arr_data = (arr_data)\n",
    "        self.x = torch.from_numpy(self.arr_data[:,1:])\n",
    "        self.x = self.x.float()\n",
    "        self.y = torch.from_numpy(self.arr_data[:,0])\n",
    "        self.y = self.y.float()\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.arr_data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index], F.one_hot(self.y[index].long(),10)\n",
    "\n",
    "\n",
    "def model_summary(model):\n",
    "    \"\"\"\n",
    "    get each layer weights, keys and shapes\n",
    "    \"\"\"\n",
    "    assert isinstance(model,nn.Module)==True\n",
    "\n",
    "    model_keys = []\n",
    "    model_shapes = []\n",
    "    for name, params in model.named_parameters():\n",
    "        if name.split(\".\")[1]==\"weight\":\n",
    "            model_keys.append(name.split(\".\")[0])\n",
    "            model_shapes.append(params.shape[0]*params.shape[1])\n",
    "\n",
    "    return model_keys,model_shapes\n",
    "\n",
    "\n",
    "\n",
    "def get_weights(model:nn.Module,model_dict):\n",
    "    \"\"\"\n",
    "    check whether model is instance of nn.Module\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if name.split(\".\")[1]==\"weight\":\n",
    "            index = name.split(\".\")[0]\n",
    "            temp_tensor = param.clone()\n",
    "            temp_weights = temp_tensor.detach().cpu().numpy()\n",
    "            temp_weights = temp_weights.reshape(-1,)\n",
    "            \n",
    "            model_dict[f'{index}'] = np.vstack((model_dict[f'{index}'],temp_weights))\n",
    "\n",
    "    return model_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_weights(model:nn.Module,name):\n",
    "    \"\"\"\n",
    "    check whether model is instance of nn.Module\n",
    "    \"\"\"\n",
    "    W = []\n",
    "    for layer, param in model.named_parameters():\n",
    "        if layer.split(\".\")[1]==\"weight\":\n",
    "            # index = name.split(\".\")[0]\n",
    "            temp_tensor = param.clone()\n",
    "            temp_weights = temp_tensor.detach().cpu().numpy()\n",
    "            temp_weights = temp_weights.reshape(-1)\n",
    "            W.extend(temp_weights.tolist())\n",
    "\n",
    "    with open(name,'a',newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerow(W)\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_accuracy(model:nn.Module, name,dataloader,device,save_weights = False):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data,targets in dataloader:\n",
    "            data = data.to(device)\n",
    "            \n",
    "            targets = targets.to(device)\n",
    "            outputs = model(data)\n",
    "            predictions = torch.argmax(outputs,1)\n",
    "            targets = torch.argmax(targets,1)\n",
    "            n_samples += targets.shape[0]\n",
    "            n_correct += (predictions == targets).sum().tolist()\n",
    "\n",
    "    acc = 100*n_correct/n_samples\n",
    "    if save_weights:\n",
    "        with open(name,'a',newline='') as file:\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            writer.writerow([acc])\n",
    "        file.close()\n",
    "    else:\n",
    "        return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE PROFILER CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n",
      "11500 41000\n",
      "epoch 1 / 50, step 200/230, loss = 0.0882\n",
      "0\n",
      "train 20.269396918262526\n",
      "test 20.33965558289025\n",
      "epoch 2 / 50, step 200/230, loss = 0.0873\n",
      "1\n",
      "train 24.796761794618863\n",
      "test 25.085310689627807\n",
      "epoch 3 / 50, step 200/230, loss = 0.0799\n",
      "2\n",
      "train 36.576074016122995\n",
      "test 37.012935481311004\n",
      "epoch 4 / 50, step 200/230, loss = 0.0801\n",
      "3\n",
      "train 46.675056974727035\n",
      "test 47.20260296801841\n",
      "epoch 5 / 50, step 200/230, loss = 0.0753\n",
      "4\n",
      "train 48.60709547943808\n",
      "test 49.09927783509245\n",
      "epoch 6 / 50, step 200/230, loss = 0.0686\n",
      "5\n",
      "train 49.341814347426784\n",
      "test 49.77382747401\n",
      "epoch 7 / 50, step 200/230, loss = 0.0613\n",
      "6\n",
      "train 54.18211503792646\n",
      "test 54.55122609316721\n",
      "epoch 8 / 50, step 200/230, loss = 0.0538\n",
      "7\n",
      "train 62.845675022959966\n",
      "test 62.96325688437425\n",
      "epoch 9 / 50, step 200/230, loss = 0.0477\n",
      "8\n",
      "train 73.13173917480187\n",
      "test 72.67677168478693\n",
      "epoch 10 / 50, step 200/230, loss = 0.0422\n",
      "9\n",
      "train 78.56049525494065\n",
      "test 78.0176176493929\n",
      "epoch 11 / 50, step 200/230, loss = 0.0357\n",
      "10\n",
      "train 82.22048368992142\n",
      "test 81.82683913975082\n",
      "epoch 12 / 50, step 200/230, loss = 0.0340\n",
      "11\n",
      "train 83.23752508588727\n",
      "test 82.64423458455678\n",
      "epoch 13 / 50, step 200/230, loss = 0.0296\n",
      "12\n",
      "train 84.80900710908534\n",
      "test 84.37425601142766\n",
      "epoch 14 / 50, step 200/230, loss = 0.0226\n",
      "13\n",
      "train 87.83632096329808\n",
      "test 87.59622252202206\n",
      "epoch 15 / 50, step 200/230, loss = 0.0179\n",
      "14\n",
      "train 89.44181774890302\n",
      "test 89.22307753352908\n",
      "epoch 16 / 50, step 200/230, loss = 0.0237\n",
      "15\n",
      "train 90.39763257253648\n",
      "test 89.89762717244663\n",
      "epoch 17 / 50, step 200/230, loss = 0.0208\n",
      "16\n",
      "train 90.88744515119562\n",
      "test 90.135703515594\n",
      "epoch 18 / 50, step 200/230, loss = 0.0142\n",
      "17\n",
      "train 91.57454335181468\n",
      "test 90.95309896039997\n",
      "epoch 19 / 50, step 200/230, loss = 0.0182\n",
      "18\n",
      "train 91.49971087451954\n",
      "test 90.75470200777715\n",
      "epoch 20 / 50, step 200/230, loss = 0.0157\n",
      "19\n",
      "train 90.99969386713833\n",
      "test 90.42933100547576\n",
      "epoch 21 / 50, step 200/230, loss = 0.0100\n",
      "20\n",
      "train 91.61876254294364\n",
      "test 90.92135544798032\n",
      "epoch 22 / 50, step 200/230, loss = 0.0202\n",
      "21\n",
      "train 91.86026735603252\n",
      "test 91.1673676692326\n",
      "epoch 23 / 50, step 200/230, loss = 0.0155\n",
      "22\n",
      "train 91.68679206775741\n",
      "test 90.80231727640664\n",
      "epoch 24 / 50, step 200/230, loss = 0.0159\n",
      "23\n",
      "train 92.12558250280622\n",
      "test 91.53241806205857\n",
      "epoch 25 / 50, step 200/230, loss = 0.0163\n",
      "24\n",
      "train 92.38069322085785\n",
      "test 91.5562256963733\n",
      "epoch 26 / 50, step 200/230, loss = 0.0160\n",
      "25\n",
      "train 92.29905779108132\n",
      "test 91.51654630584875\n",
      "epoch 27 / 50, step 200/230, loss = 0.0150\n",
      "26\n",
      "train 92.2752474573965\n",
      "test 91.89746845488453\n",
      "epoch 28 / 50, step 200/230, loss = 0.0160\n",
      "27\n",
      "train 92.19361202761999\n",
      "test 91.7466867708912\n",
      "epoch 29 / 50, step 200/230, loss = 0.0112\n",
      "28\n",
      "train 92.37729174461717\n",
      "test 91.90540433298945\n",
      "epoch 30 / 50, step 200/230, loss = 0.0146\n",
      "29\n",
      "train 91.95210721453111\n",
      "test 91.66732798984208\n",
      "epoch 31 / 50, step 200/230, loss = 0.0084\n",
      "30\n",
      "train 92.61199360522467\n",
      "test 91.96889135782874\n",
      "epoch 32 / 50, step 200/230, loss = 0.0133\n",
      "31\n",
      "train 92.17660464641655\n",
      "test 91.34195698754067\n",
      "epoch 33 / 50, step 200/230, loss = 0.0144\n",
      "32\n",
      "train 92.87050579951699\n",
      "test 92.19109594476629\n",
      "epoch 34 / 50, step 200/230, loss = 0.0088\n",
      "33\n",
      "train 92.58138031905847\n",
      "test 91.65939211173716\n",
      "epoch 35 / 50, step 200/230, loss = 0.0122\n",
      "34\n",
      "train 92.28885336235926\n",
      "test 92.01650662645822\n",
      "epoch 36 / 50, step 200/230, loss = 0.0118\n",
      "35\n",
      "train 92.99976189666314\n",
      "test 92.397428775494\n",
      "epoch 37 / 50, step 200/230, loss = 0.0123\n",
      "36\n",
      "train 92.96914861049696\n",
      "test 92.04825013887786\n",
      "epoch 38 / 50, step 200/230, loss = 0.0084\n",
      "37\n",
      "train 93.28208442464029\n",
      "test 92.57995397190699\n",
      "epoch 39 / 50, step 200/230, loss = 0.0118\n",
      "38\n",
      "train 92.88751318072043\n",
      "test 92.11173716371717\n",
      "epoch 40 / 50, step 200/230, loss = 0.0146\n",
      "39\n",
      "train 93.11881356508725\n",
      "test 92.11173716371717\n",
      "epoch 41 / 50, step 200/230, loss = 0.0078\n",
      "40\n",
      "train 93.36031837817613\n",
      "test 92.38155701928419\n",
      "epoch 42 / 50, step 200/230, loss = 0.0090\n",
      "41\n",
      "train 93.31609918704717\n",
      "test 92.63550511864138\n",
      "epoch 43 / 50, step 200/230, loss = 0.0122\n",
      "42\n",
      "train 92.85349841831355\n",
      "test 92.11967304182208\n",
      "epoch 44 / 50, step 200/230, loss = 0.0067\n",
      "43\n",
      "train 93.50998333276642\n",
      "test 92.74660741211015\n",
      "epoch 45 / 50, step 200/230, loss = 0.0089\n",
      "44\n",
      "train 93.55080104765469\n",
      "test 92.61169748432664\n",
      "epoch 46 / 50, step 200/230, loss = 0.0073\n",
      "45\n",
      "train 93.97938705398143\n",
      "test 92.81803031505436\n",
      "epoch 47 / 50, step 200/230, loss = 0.0083\n",
      "46\n",
      "train 93.29228885336236\n",
      "test 92.5244028251726\n",
      "epoch 48 / 50, step 200/230, loss = 0.0095\n",
      "47\n",
      "train 92.96234565801558\n",
      "test 92.24664709150068\n",
      "epoch 49 / 50, step 200/230, loss = 0.0097\n",
      "48\n",
      "train 93.52018776148849\n",
      "test 92.50853106896278\n",
      "epoch 50 / 50, step 200/230, loss = 0.0092\n",
      "49\n",
      "train 93.61883057246845\n",
      "test 92.7783509245298\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torch_custom import *\n",
    "import tqdm \n",
    "\n",
    "\n",
    "from torch_custom import *\n",
    "import tqdm \n",
    "\n",
    "# start from 1\n",
    "initialization = 1\n",
    "torch.manual_seed(initialization)\n",
    "np.random.seed(initialization)\n",
    "\n",
    "\n",
    "# initialization = 5 seed = 0\n",
    "\n",
    "name = f\"weights{initialization}.csv\"\n",
    "train_acc = f\"train_accuracies{initialization}.csv\"\n",
    "test_acc = f\"test_accuracies{initialization}.csv\"\n",
    "\n",
    "datapath = r\"mnist.csv\"\n",
    "\n",
    "\"\"\"\n",
    "MODEL INTITIALIZATION AND SETTING UP THE HYPERPARAMETERS\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "model = MNIST(784,10).to(device) #change the model definition from the torch_custom file to test on other datasets\n",
    "train_size = 0.7\n",
    "datasets = MNISTDataset(datapath,train=True,size=train_size) #change the dataset class from the torch_custom file\n",
    "test_datasets = MNISTDataset(datapath,train=False,size=train_size)\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset=datasets,batch_size=batch_size,shuffle=True)\n",
    "testloader = DataLoader(dataset=test_datasets,batch_size=batch_size)\n",
    "\n",
    "num_epochs = 50\n",
    "learning_rate = 0.5\n",
    "\n",
    "n_total_steps = len(dataloader)\n",
    "print(n_total_steps)\n",
    "\n",
    "torch.save(model.state_dict(), f\"model_{initialization}.pth\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "\n",
    "w_num = 0\n",
    "model_keys, model_shapes = model_summary(model)\n",
    "for shapes in model_shapes:\n",
    "    w_num += shapes             #shapes[0]*shapes[1]\n",
    "#weights = np.zeros(shape=(n_total_steps*num_epochs,w_num))\n",
    "print(n_total_steps*num_epochs,w_num)\n",
    "\n",
    "acc = []\n",
    "for i in range(0,25):\n",
    "    r = 45*(i+1)+5\n",
    "    acc.append(r)\n",
    "\n",
    "\n",
    "j = 0\n",
    "\n",
    "\"\"\"\n",
    "TRAINING AND VALIDATION LOOPS \n",
    "\"\"\"\n",
    "prof = torch.profiler.profile(\n",
    "        schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('/log/torch_custom'),\n",
    "        record_shapes=True,profile_memory=True,\n",
    "        with_stack=True)\n",
    "prof.start()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    " \n",
    "    model.train()\n",
    "    for i,(data,targets) in enumerate(dataloader):\n",
    "#       weights[j] = get_weights(model=model)\n",
    "        j+=1\n",
    "        data = data.to(device)\n",
    "        temp_targets = targets\n",
    "        targets = targets.float()\n",
    "\n",
    "        targets = targets.to(device)\n",
    "   \n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs,targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if(i+1)%200==0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
    "#        if epoch*n_total_steps+i in acc:\n",
    "#            get_accuracy(model=model,name =train_acc,dataloader=dataloader,device=device,save_weights=True)\n",
    "#            get_accuracy(model=model,name = test_acc,dataloader=testloader,device=device,save_weights=True)\n",
    "    print(epoch)\n",
    "    print(\"train\",get_accuracy(model=model,name =train_acc,device=device,dataloader=dataloader))\n",
    "    print(\"test\",get_accuracy(model=model,name = test_acc,device=device,dataloader=testloader))\n",
    "\n",
    "\n",
    "prof.stop()\n",
    "#np.savetxt(f\"weights{initialization}.csv\",weights,delimiter=',')\n",
    "#weights = pd.DataFrame(weights)\n",
    "#weights.to_csv(f\"weights{initialization}.csv\",header=False,index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "       model_inference        17.95%     337.000us       100.00%       1.877ms       1.877ms             1  \n",
      "          aten::linear         1.60%      30.000us        78.96%       1.482ms     494.000us             3  \n",
      "           aten::addmm        68.30%       1.282ms        71.34%       1.339ms     446.333us             3  \n",
      "               aten::t         3.52%      66.000us         6.02%     113.000us      37.667us             3  \n",
      "       aten::transpose         1.70%      32.000us         2.50%      47.000us      15.667us             3  \n",
      "         aten::sigmoid         2.29%      43.000us         2.29%      43.000us      21.500us             2  \n",
      "           aten::copy_         2.13%      40.000us         2.13%      40.000us      13.333us             3  \n",
      "      aten::as_strided         0.85%      16.000us         0.85%      16.000us       2.667us             6  \n",
      "          aten::expand         0.80%      15.000us         0.85%      16.000us       5.333us             3  \n",
      "         aten::softmax         0.21%       4.000us         0.80%      15.000us      15.000us             1  \n",
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.877ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-05-04 12:43:07 23102:23102 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-05-04 12:43:07 23102:23102 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-05-04 12:43:07 23102:23102 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = torch.randn(1, 784)\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "       model_inference        17.95%     337.000us       100.00%       1.877ms       1.877ms             1  \n",
      "          aten::linear         1.60%      30.000us        78.96%       1.482ms     494.000us             3  \n",
      "               aten::t         3.52%      66.000us         6.02%     113.000us      37.667us             3  \n",
      "       aten::transpose         1.70%      32.000us         2.50%      47.000us      15.667us             3  \n",
      "      aten::as_strided         0.85%      16.000us         0.85%      16.000us       2.667us             6  \n",
      "           aten::addmm        68.30%       1.282ms        71.34%       1.339ms     446.333us             3  \n",
      "          aten::expand         0.80%      15.000us         0.85%      16.000us       5.333us             3  \n",
      "           aten::copy_         2.13%      40.000us         2.13%      40.000us      13.333us             3  \n",
      "    aten::resolve_conj         0.05%       1.000us         0.05%       1.000us       0.167us             6  \n",
      "         aten::sigmoid         2.29%      43.000us         2.29%      43.000us      21.500us             2  \n",
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.877ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chaos",
   "language": "python",
   "name": "chaos_net"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
